{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÑ Multi-Source Data Pipeline: ETL Framework\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This project demonstrates a production-grade ETL (Extract, Transform, Load) pipeline for processing data from multiple sources. The pipeline handles real-world challenges including:\n",
    "\n",
    "- **Multiple data formats**: CSV, JSON, and API responses\n",
    "- **Data quality issues**: Missing values, duplicates, outliers, invalid formats\n",
    "- **Complex transformations**: Cleaning, validation, enrichment, aggregation\n",
    "- **Professional software design**: Modular, testable, well-documented code\n",
    "\n",
    "**Business Scenario:** An e-commerce company needs to consolidate data from:\n",
    "1. Sales transactions (CSV)\n",
    "2. Product catalog (JSON)\n",
    "3. Customer database (CSV)\n",
    "4. Inventory system (CSV)\n",
    "5. External market data (API/JSON)\n",
    "6. Supplier information (JSON)\n",
    "\n",
    "**Deliverables:**\n",
    "- Cleaned, validated datasets\n",
    "- Integrated data warehouse tables\n",
    "- Data quality reports\n",
    "- Executive summary analytics\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Alexy Louis  \n",
    "**Email:** alexy.louis.scholar@gmail.com  \n",
    "**LinkedIn:** [linkedin.com/in/alexy-louis-19a5a9262](https://www.linkedin.com/in/alexy-louis-19a5a9262/)  \n",
    "**GitHub:** [github.com/Smooth-Cactus0](https://github.com/Smooth-Cactus0)  \n",
    "**Date:** December 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup & Configuration](#1.-Setup-&-Configuration)\n",
    "2. [Data Extraction](#2.-Data-Extraction)\n",
    "3. [Data Quality Assessment](#3.-Data-Quality-Assessment)\n",
    "4. [Data Validation](#4.-Data-Validation)\n",
    "5. [Data Transformation](#5.-Data-Transformation)\n",
    "6. [Data Integration](#6.-Data-Integration)\n",
    "7. [Analytics & Insights](#7.-Analytics-&-Insights)\n",
    "8. [Export & Reporting](#8.-Export-&-Reporting)\n",
    "9. [Pipeline Orchestration](#9.-Pipeline-Orchestration)\n",
    "10. [Conclusions](#10.-Conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "# Import our custom modules\n",
    "from data_loader import DataLoader\n",
    "from data_validator import DataValidator, ValidationSeverity\n",
    "from data_transformer import DataTransformer\n",
    "from pipeline_orchestrator import ETLPipeline, PipelineStatus\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Custom color palette\n",
    "COLORS = {\n",
    "    'primary': '#2E86AB',\n",
    "    'secondary': '#A23B72',\n",
    "    'success': '#27AE60',\n",
    "    'danger': '#E74C3C',\n",
    "    'warning': '#F39C12',\n",
    "    'info': '#17A2B8'\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(f\"\\nPython version: {sys.version.split()[0]}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "RAW_DATA_PATH = Path('../data/raw')\n",
    "EXTERNAL_DATA_PATH = Path('../data/external')\n",
    "PROCESSED_DATA_PATH = Path('../data/processed')\n",
    "IMAGES_PATH = Path('../images')\n",
    "\n",
    "# Ensure directories exist\n",
    "PROCESSED_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üìÅ Data Paths:\")\n",
    "print(f\"   Raw data: {RAW_DATA_PATH}\")\n",
    "print(f\"   External data: {EXTERNAL_DATA_PATH}\")\n",
    "print(f\"   Processed data: {PROCESSED_DATA_PATH}\")\n",
    "\n",
    "# List available files\n",
    "print(\"\\nüìÑ Available Data Files:\")\n",
    "for f in RAW_DATA_PATH.glob('*'):\n",
    "    size_kb = f.stat().st_size / 1024\n",
    "    print(f\"   {f.name} ({size_kb:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Extraction\n",
    "\n",
    "Load data from multiple sources using our custom `DataLoader` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DataLoader\n",
    "loader = DataLoader(base_path=str(RAW_DATA_PATH))\n",
    "external_loader = DataLoader(base_path=str(EXTERNAL_DATA_PATH))\n",
    "\n",
    "print(\"DataLoader initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Sales Transactions (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sales transactions\n",
    "sales_df = loader.load_csv(\n",
    "    'sales_transactions.csv',\n",
    "    parse_dates=['transaction_date']\n",
    ")\n",
    "\n",
    "print(\"\\nSALES TRANSACTIONS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Shape: {sales_df.shape}\")\n",
    "print(f\"Date range: {sales_df['transaction_date'].min()} to {sales_df['transaction_date'].max()}\")\n",
    "print(f\"\\nColumns: {list(sales_df.columns)}\")\n",
    "sales_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Product Catalog (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load product catalog\n",
    "products_df = loader.load_json(\n",
    "    'product_catalog.json',\n",
    "    normalize=True,\n",
    "    record_path='products'\n",
    ")\n",
    "\n",
    "print(\"\\nPRODUCT CATALOG\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Shape: {products_df.shape}\")\n",
    "print(f\"Categories: {products_df['category'].nunique()}\")\n",
    "products_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Customer Data (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load customer data\n",
    "customers_df = loader.load_csv(\n",
    "    'customers.csv',\n",
    "    parse_dates=['signup_date']\n",
    ")\n",
    "\n",
    "print(\"\\nCUSTOMER DATA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Shape: {customers_df.shape}\")\n",
    "print(f\"Segments: {customers_df['segment'].value_counts().to_dict()}\")\n",
    "customers_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Inventory Data (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inventory data\n",
    "inventory_df = loader.load_csv('inventory.csv')\n",
    "\n",
    "print(\"\\nINVENTORY DATA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Shape: {inventory_df.shape}\")\n",
    "print(f\"Warehouses: {inventory_df['warehouse_id'].unique()}\")\n",
    "inventory_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 External Market Data (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load external market data (simulated API response)\n",
    "market_data = external_loader.load_json('market_data.json', normalize=False)\n",
    "\n",
    "print(\"\\nMARKET DATA (External API)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"API Version: {market_data.get('api_version')}\")\n",
    "print(f\"Data Source: {market_data.get('data_source')}\")\n",
    "print(f\"\\nMarket Trends:\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "market_trends_df = pd.DataFrame(market_data['market_trends'])\n",
    "market_trends_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Supplier Data (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load supplier data\n",
    "suppliers_df = loader.load_json(\n",
    "    'suppliers.json',\n",
    "    normalize=True,\n",
    "    record_path='suppliers'\n",
    ")\n",
    "\n",
    "print(\"\\nSUPPLIER DATA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Shape: {suppliers_df.shape}\")\n",
    "print(f\"Active suppliers: {suppliers_df['active'].sum()}\")\n",
    "suppliers_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Load Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined load summary\n",
    "load_summary = pd.concat([\n",
    "    loader.get_load_summary(),\n",
    "    external_loader.get_load_summary()\n",
    "], ignore_index=True)\n",
    "\n",
    "print(\"\\nüìä DATA EXTRACTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(load_summary.to_string(index=False))\n",
    "\n",
    "total_rows = load_summary['rows'].sum()\n",
    "total_time = load_summary['load_time'].sum()\n",
    "print(f\"\\nTotal rows loaded: {total_rows:,}\")\n",
    "print(f\"Total load time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Quality Assessment\n",
    "\n",
    "Before cleaning, let's assess the quality of our raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(df, name):\n",
    "    \"\"\"Generate comprehensive data quality report.\"\"\"\n",
    "    report = {\n",
    "        'Dataset': name,\n",
    "        'Rows': len(df),\n",
    "        'Columns': len(df.columns),\n",
    "        'Duplicates': df.duplicated().sum(),\n",
    "        'Duplicate %': round(df.duplicated().sum() / len(df) * 100, 2),\n",
    "        'Total Nulls': df.isnull().sum().sum(),\n",
    "        'Null %': round(df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100, 2),\n",
    "        'Memory (MB)': round(df.memory_usage(deep=True).sum() / 1024 / 1024, 2)\n",
    "    }\n",
    "    return report\n",
    "\n",
    "# Assess all datasets\n",
    "datasets = {\n",
    "    'Sales': sales_df,\n",
    "    'Products': products_df,\n",
    "    'Customers': customers_df,\n",
    "    'Inventory': inventory_df,\n",
    "    'Suppliers': suppliers_df\n",
    "}\n",
    "\n",
    "quality_reports = [assess_data_quality(df, name) for name, df in datasets.items()]\n",
    "quality_df = pd.DataFrame(quality_reports)\n",
    "\n",
    "print(\"üìã DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\"*80)\n",
    "print(quality_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed null analysis for Sales data\n",
    "print(\"\\nüîç DETAILED NULL ANALYSIS - SALES DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "null_counts = sales_df.isnull().sum()\n",
    "null_pct = (sales_df.isnull().sum() / len(sales_df) * 100).round(2)\n",
    "null_analysis = pd.DataFrame({\n",
    "    'Column': null_counts.index,\n",
    "    'Null Count': null_counts.values,\n",
    "    'Null %': null_pct.values\n",
    "}).sort_values('Null Count', ascending=False)\n",
    "\n",
    "print(null_analysis[null_analysis['Null Count'] > 0].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for data quality issues in Sales\n",
    "print(\"\\n‚ö†Ô∏è DATA QUALITY ISSUES DETECTED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Negative values\n",
    "neg_quantity = (sales_df['quantity'] < 0).sum()\n",
    "neg_price = (sales_df['unit_price'] < 0).sum()\n",
    "print(f\"\\nNegative quantities: {neg_quantity}\")\n",
    "print(f\"Negative prices: {neg_price}\")\n",
    "\n",
    "# Show examples\n",
    "if neg_quantity > 0:\n",
    "    print(\"\\nSample negative quantity records:\")\n",
    "    print(sales_df[sales_df['quantity'] < 0][['transaction_id', 'quantity', 'unit_price']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data quality\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Null percentages\n",
    "colors = [COLORS['danger'] if x > 1 else COLORS['success'] for x in quality_df['Null %']]\n",
    "bars = axes[0].bar(quality_df['Dataset'], quality_df['Null %'], color=colors, edgecolor='black')\n",
    "axes[0].set_ylabel('Null %', fontsize=12)\n",
    "axes[0].set_title('Missing Values by Dataset', fontsize=14, fontweight='bold')\n",
    "axes[0].axhline(y=1, color='red', linestyle='--', alpha=0.5, label='1% threshold')\n",
    "for bar, val in zip(bars, quality_df['Null %']):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                 f'{val}%', ha='center', fontsize=10)\n",
    "\n",
    "# Duplicate percentages\n",
    "colors = [COLORS['danger'] if x > 0.5 else COLORS['success'] for x in quality_df['Duplicate %']]\n",
    "bars = axes[1].bar(quality_df['Dataset'], quality_df['Duplicate %'], color=colors, edgecolor='black')\n",
    "axes[1].set_ylabel('Duplicate %', fontsize=12)\n",
    "axes[1].set_title('Duplicate Records by Dataset', fontsize=14, fontweight='bold')\n",
    "for bar, val in zip(bars, quality_df['Duplicate %']):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                 f'{val}%', ha='center', fontsize=10)\n",
    "\n",
    "# Row counts\n",
    "colors = plt.cm.Blues(np.linspace(0.4, 0.9, len(quality_df)))\n",
    "bars = axes[2].bar(quality_df['Dataset'], quality_df['Rows'], color=colors, edgecolor='black')\n",
    "axes[2].set_ylabel('Row Count', fontsize=12)\n",
    "axes[2].set_title('Dataset Sizes', fontsize=14, fontweight='bold')\n",
    "for bar, val in zip(bars, quality_df['Rows']):\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50, \n",
    "                 f'{val:,}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/01_data_quality_overview.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Validation\n",
    "\n",
    "Using our custom `DataValidator` to perform comprehensive validation checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validator for Sales data\n",
    "sales_validator = DataValidator(name=\"SalesValidator\")\n",
    "\n",
    "# Add validation rules\n",
    "sales_validator.add_uniqueness_check('transaction_id')\n",
    "sales_validator.add_null_check(['transaction_id', 'customer_id', 'product_id'])\n",
    "sales_validator.add_range_check('quantity', min_val=1, max_val=1000)\n",
    "sales_validator.add_range_check('unit_price', min_val=0.01, max_val=10000)\n",
    "sales_validator.add_allowed_values_check('order_status', \n",
    "    ['Completed', 'Shipped', 'Processing', 'Refunded', 'Cancelled'])\n",
    "sales_validator.add_allowed_values_check('payment_method',\n",
    "    ['Credit Card', 'Debit Card', 'PayPal', 'Apple Pay', 'Google Pay', 'Bank Transfer'])\n",
    "\n",
    "# Run validation\n",
    "print(\"\\nüîç SALES DATA VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "sales_validation_results = sales_validator.validate(sales_df)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + sales_validator.generate_report().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Customers data\n",
    "customers_validator = DataValidator(name=\"CustomersValidator\")\n",
    "\n",
    "customers_validator.add_uniqueness_check('customer_id')\n",
    "customers_validator.add_null_check(['customer_id', 'email'])\n",
    "customers_validator.add_email_check('email')\n",
    "customers_validator.add_range_check('age', min_val=0, max_val=120)\n",
    "customers_validator.add_allowed_values_check('segment', ['Bronze', 'Silver', 'Gold', 'Platinum'])\n",
    "\n",
    "print(\"\\nüîç CUSTOMER DATA VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "customers_validation_results = customers_validator.validate(customers_df)\n",
    "print(\"\\n\" + customers_validator.generate_report().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Inventory data\n",
    "inventory_validator = DataValidator(name=\"InventoryValidator\")\n",
    "\n",
    "inventory_validator.add_null_check(['product_id', 'warehouse_id'])\n",
    "inventory_validator.add_range_check('quantity_on_hand', min_val=0)\n",
    "inventory_validator.add_allowed_values_check('warehouse_id', ['WH-EAST', 'WH-WEST', 'WH-CENTRAL'])\n",
    "\n",
    "print(\"\\nüîç INVENTORY DATA VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "inventory_validation_results = inventory_validator.validate(inventory_df)\n",
    "print(\"\\n\" + inventory_validator.generate_report().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Summary Visualization\n",
    "validation_summary = {\n",
    "    'Sales': sales_validator.get_summary(),\n",
    "    'Customers': customers_validator.get_summary(),\n",
    "    'Inventory': inventory_validator.get_summary()\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "datasets = list(validation_summary.keys())\n",
    "passed = [validation_summary[d]['passed'] for d in datasets]\n",
    "failed = [validation_summary[d]['failed'] for d in datasets]\n",
    "\n",
    "x = np.arange(len(datasets))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, passed, width, label='Passed', color=COLORS['success'], edgecolor='black')\n",
    "bars2 = ax.bar(x + width/2, failed, width, label='Failed', color=COLORS['danger'], edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Dataset', fontsize=12)\n",
    "ax.set_ylabel('Number of Checks', fontsize=12)\n",
    "ax.set_title('Validation Results by Dataset', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(datasets)\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "            str(int(bar.get_height())), ha='center', fontsize=11, fontweight='bold')\n",
    "for bar in bars2:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "            str(int(bar.get_height())), ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/02_validation_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Data Transformation\n",
    "\n",
    "Clean and transform data using our `DataTransformer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Sales Data\n",
    "print(\"\\nüîÑ TRANSFORMING SALES DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sales_transformer = DataTransformer(sales_df, name=\"SalesTransformer\")\n",
    "\n",
    "sales_clean = (sales_transformer\n",
    "    # Remove duplicates\n",
    "    .remove_duplicates(['transaction_id'])\n",
    "    \n",
    "    # Fix negative values - remove invalid records\n",
    "    .filter_rows(lambda df: df['quantity'] > 0)\n",
    "    .filter_rows(lambda df: df['unit_price'] > 0)\n",
    "    \n",
    "    # Handle null values\n",
    "    .fill_nulls('payment_method', strategy='mode')\n",
    "    .fill_nulls('shipping_cost', strategy='median')\n",
    "    .fill_nulls('sales_channel', strategy='mode')\n",
    "    \n",
    "    # Add calculated columns\n",
    "    .add_column('subtotal', lambda df: df['quantity'] * df['unit_price'])\n",
    "    .add_column('total', lambda df: df['quantity'] * df['unit_price'] - df['discount_amount'] + df['shipping_cost'])\n",
    "    .add_column('transaction_month', lambda df: df['transaction_date'].dt.to_period('M').astype(str))\n",
    "    .add_column('transaction_day_of_week', lambda df: df['transaction_date'].dt.day_name())\n",
    "    \n",
    "    .get_result()\n",
    ")\n",
    "\n",
    "print(\"\\nTransformation Log:\")\n",
    "print(sales_transformer.get_transformation_log().to_string(index=False))\n",
    "print(f\"\\nSummary: {sales_transformer.get_summary()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Customer Data\n",
    "print(\"\\nüîÑ TRANSFORMING CUSTOMER DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "customers_transformer = DataTransformer(customers_df, name=\"CustomersTransformer\")\n",
    "\n",
    "customers_clean = (customers_transformer\n",
    "    # Remove duplicates\n",
    "    .remove_duplicates(['customer_id'])\n",
    "    \n",
    "    # Fix invalid ages\n",
    "    .filter_rows(lambda df: (df['age'] > 0) & (df['age'] < 120))\n",
    "    \n",
    "    # Fix invalid emails (replace with placeholder)\n",
    "    .apply_custom(\n",
    "        lambda df: df.assign(\n",
    "            email=df['email'].apply(\n",
    "                lambda x: x if '@' in str(x) and '.' in str(x) else 'invalid@placeholder.com'\n",
    "            )\n",
    "        ),\n",
    "        description=\"Fix invalid emails\"\n",
    "    )\n",
    "    \n",
    "    # Fill missing regions\n",
    "    .fill_nulls('region', strategy='mode')\n",
    "    \n",
    "    # Standardize text fields\n",
    "    .standardize_text(['first_name', 'last_name'], lowercase=False, strip=True)\n",
    "    \n",
    "    # Add customer tenure\n",
    "    .add_column('tenure_days', lambda df: (pd.Timestamp.now() - pd.to_datetime(df['signup_date'])).dt.days)\n",
    "    .add_column('tenure_years', lambda df: (df['tenure_days'] / 365).round(1))\n",
    "    \n",
    "    .get_result()\n",
    ")\n",
    "\n",
    "print(\"\\nTransformation Log:\")\n",
    "print(customers_transformer.get_transformation_log().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Inventory Data\n",
    "print(\"\\nüîÑ TRANSFORMING INVENTORY DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "inventory_transformer = DataTransformer(inventory_df, name=\"InventoryTransformer\")\n",
    "\n",
    "inventory_clean = (inventory_transformer\n",
    "    # Remove invalid warehouse IDs\n",
    "    .filter_by_values('warehouse_id', ['WH-EAST', 'WH-WEST', 'WH-CENTRAL'], keep=True)\n",
    "    \n",
    "    # Fix negative quantities\n",
    "    .apply_custom(\n",
    "        lambda df: df.assign(quantity_on_hand=df['quantity_on_hand'].clip(lower=0)),\n",
    "        description=\"Fix negative quantities\"\n",
    "    )\n",
    "    \n",
    "    # Add calculated columns\n",
    "    .add_column('available_quantity', \n",
    "                lambda df: df['quantity_on_hand'] - df['quantity_reserved'])\n",
    "    .add_column('needs_reorder',\n",
    "                lambda df: df['quantity_on_hand'] <= df['reorder_point'])\n",
    "    .add_column('inventory_value',\n",
    "                lambda df: df['quantity_on_hand'] * df['unit_cost'])\n",
    "    \n",
    "    .get_result()\n",
    ")\n",
    "\n",
    "print(\"\\nTransformation Log:\")\n",
    "print(inventory_transformer.get_transformation_log().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all transformations\n",
    "print(\"\\nüìä TRANSFORMATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "transform_summary = pd.DataFrame([\n",
    "    {'Dataset': 'Sales', **sales_transformer.get_summary()},\n",
    "    {'Dataset': 'Customers', **customers_transformer.get_summary()},\n",
    "    {'Dataset': 'Inventory', **inventory_transformer.get_summary()}\n",
    "])\n",
    "\n",
    "print(transform_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Data Integration\n",
    "\n",
    "Merge and integrate data from multiple sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich Sales with Product information\n",
    "print(\"\\nüîó DATA INTEGRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Select relevant product columns\n",
    "product_cols = ['product_id', 'product_name', 'category', 'subcategory', \n",
    "                'base_price', 'cost_price', 'brand']\n",
    "\n",
    "# Merge sales with products\n",
    "sales_enriched = sales_clean.merge(\n",
    "    products_df[product_cols],\n",
    "    on='product_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Sales + Products merged: {len(sales_enriched):,} rows\")\n",
    "\n",
    "# Merge with customer information\n",
    "customer_cols = ['customer_id', 'segment', 'region', 'city', 'tenure_years']\n",
    "\n",
    "sales_enriched = sales_enriched.merge(\n",
    "    customers_clean[customer_cols],\n",
    "    on='customer_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Sales + Customers merged: {len(sales_enriched):,} rows\")\n",
    "\n",
    "# Add profit margin calculation\n",
    "sales_enriched['profit'] = sales_enriched['total'] - (sales_enriched['cost_price'] * sales_enriched['quantity'])\n",
    "sales_enriched['profit_margin'] = (sales_enriched['profit'] / sales_enriched['total'] * 100).round(2)\n",
    "\n",
    "print(f\"\\nFinal enriched dataset: {sales_enriched.shape}\")\n",
    "sales_enriched.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inventory summary by product\n",
    "inventory_summary = inventory_clean.groupby('product_id').agg({\n",
    "    'quantity_on_hand': 'sum',\n",
    "    'quantity_reserved': 'sum',\n",
    "    'available_quantity': 'sum',\n",
    "    'inventory_value': 'sum',\n",
    "    'needs_reorder': 'any'\n",
    "}).reset_index()\n",
    "\n",
    "inventory_summary.columns = ['product_id', 'total_stock', 'total_reserved', \n",
    "                             'total_available', 'total_inventory_value', 'any_warehouse_needs_reorder']\n",
    "\n",
    "print(\"\\nüì¶ INVENTORY SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Products in inventory: {len(inventory_summary):,}\")\n",
    "print(f\"Total inventory value: ${inventory_summary['total_inventory_value'].sum():,.2f}\")\n",
    "print(f\"Products needing reorder: {inventory_summary['any_warehouse_needs_reorder'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Analytics & Insights\n",
    "\n",
    "Generate business insights from the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales Analytics\n",
    "print(\"\\nüìà SALES ANALYTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Filter to completed orders only\n",
    "completed_sales = sales_enriched[sales_enriched['order_status'] == 'Completed']\n",
    "\n",
    "print(f\"\\nTotal Transactions: {len(completed_sales):,}\")\n",
    "print(f\"Total Revenue: ${completed_sales['total'].sum():,.2f}\")\n",
    "print(f\"Total Profit: ${completed_sales['profit'].sum():,.2f}\")\n",
    "print(f\"Average Order Value: ${completed_sales['total'].mean():,.2f}\")\n",
    "print(f\"Average Profit Margin: {completed_sales['profit_margin'].mean():.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales by Category\n",
    "category_sales = completed_sales.groupby('category').agg({\n",
    "    'transaction_id': 'count',\n",
    "    'total': 'sum',\n",
    "    'profit': 'sum',\n",
    "    'quantity': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "category_sales.columns = ['Transactions', 'Revenue', 'Profit', 'Units Sold']\n",
    "category_sales['Avg Order Value'] = (category_sales['Revenue'] / category_sales['Transactions']).round(2)\n",
    "category_sales = category_sales.sort_values('Revenue', ascending=False)\n",
    "\n",
    "print(\"\\nüìä SALES BY CATEGORY\")\n",
    "print(\"=\"*60)\n",
    "print(category_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales by Customer Segment\n",
    "segment_sales = completed_sales.groupby('segment').agg({\n",
    "    'transaction_id': 'count',\n",
    "    'total': 'sum',\n",
    "    'customer_id': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "segment_sales.columns = ['Transactions', 'Revenue', 'Unique Customers']\n",
    "segment_sales['Revenue per Customer'] = (segment_sales['Revenue'] / segment_sales['Unique Customers']).round(2)\n",
    "segment_sales = segment_sales.sort_values('Revenue', ascending=False)\n",
    "\n",
    "print(\"\\nüë• SALES BY CUSTOMER SEGMENT\")\n",
    "print(\"=\"*60)\n",
    "print(segment_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive analytics visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Revenue by Category\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(category_sales)))\n",
    "axes[0, 0].pie(category_sales['Revenue'], labels=category_sales.index, autopct='%1.1f%%',\n",
    "               colors=colors, startangle=90)\n",
    "axes[0, 0].set_title('Revenue Distribution by Category', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. Sales by Channel\n",
    "channel_sales = completed_sales.groupby('sales_channel')['total'].sum().sort_values(ascending=True)\n",
    "colors = plt.cm.Blues(np.linspace(0.4, 0.9, len(channel_sales)))\n",
    "bars = axes[0, 1].barh(channel_sales.index, channel_sales.values, color=colors, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Revenue ($)', fontsize=12)\n",
    "axes[0, 1].set_title('Revenue by Sales Channel', fontsize=14, fontweight='bold')\n",
    "for bar, val in zip(bars, channel_sales.values):\n",
    "    axes[0, 1].text(bar.get_width() + 5000, bar.get_y() + bar.get_height()/2, \n",
    "                    f'${val:,.0f}', va='center', fontsize=10)\n",
    "\n",
    "# 3. Monthly Revenue Trend\n",
    "monthly_revenue = completed_sales.groupby('transaction_month')['total'].sum()\n",
    "axes[1, 0].plot(monthly_revenue.index, monthly_revenue.values, marker='o', \n",
    "                linewidth=2, color=COLORS['primary'], markersize=8)\n",
    "axes[1, 0].fill_between(monthly_revenue.index, monthly_revenue.values, alpha=0.3, color=COLORS['primary'])\n",
    "axes[1, 0].set_xlabel('Month', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Revenue ($)', fontsize=12)\n",
    "axes[1, 0].set_title('Monthly Revenue Trend', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Customer Segment Performance\n",
    "segment_order = ['Platinum', 'Gold', 'Silver', 'Bronze']\n",
    "segment_sales_ordered = segment_sales.reindex(segment_order)\n",
    "x = np.arange(len(segment_order))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[1, 1].bar(x - width/2, segment_sales_ordered['Transactions'], width, \n",
    "                       label='Transactions', color=COLORS['primary'], edgecolor='black')\n",
    "axes[1, 1].set_ylabel('Transactions', fontsize=12, color=COLORS['primary'])\n",
    "\n",
    "ax2 = axes[1, 1].twinx()\n",
    "bars2 = ax2.bar(x + width/2, segment_sales_ordered['Revenue per Customer'], width,\n",
    "                label='Revenue/Customer', color=COLORS['secondary'], edgecolor='black')\n",
    "ax2.set_ylabel('Revenue per Customer ($)', fontsize=12, color=COLORS['secondary'])\n",
    "\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(segment_order)\n",
    "axes[1, 1].set_title('Customer Segment Performance', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/03_sales_analytics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Export & Reporting\n",
    "\n",
    "Save processed data and generate reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export cleaned datasets\n",
    "print(\"\\nüíæ EXPORTING PROCESSED DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save cleaned sales\n",
    "sales_clean.to_csv(PROCESSED_DATA_PATH / 'sales_cleaned.csv', index=False)\n",
    "print(f\"‚úÖ Saved: sales_cleaned.csv ({len(sales_clean):,} rows)\")\n",
    "\n",
    "# Save enriched sales\n",
    "sales_enriched.to_csv(PROCESSED_DATA_PATH / 'sales_enriched.csv', index=False)\n",
    "print(f\"‚úÖ Saved: sales_enriched.csv ({len(sales_enriched):,} rows)\")\n",
    "\n",
    "# Save cleaned customers\n",
    "customers_clean.to_csv(PROCESSED_DATA_PATH / 'customers_cleaned.csv', index=False)\n",
    "print(f\"‚úÖ Saved: customers_cleaned.csv ({len(customers_clean):,} rows)\")\n",
    "\n",
    "# Save cleaned inventory\n",
    "inventory_clean.to_csv(PROCESSED_DATA_PATH / 'inventory_cleaned.csv', index=False)\n",
    "print(f\"‚úÖ Saved: inventory_cleaned.csv ({len(inventory_clean):,} rows)\")\n",
    "\n",
    "# Save inventory summary\n",
    "inventory_summary.to_csv(PROCESSED_DATA_PATH / 'inventory_summary.csv', index=False)\n",
    "print(f\"‚úÖ Saved: inventory_summary.csv ({len(inventory_summary):,} rows)\")\n",
    "\n",
    "# Save category analytics\n",
    "category_sales.to_csv(PROCESSED_DATA_PATH / 'category_analytics.csv')\n",
    "print(f\"‚úÖ Saved: category_analytics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pipeline report\n",
    "pipeline_report = {\n",
    "    'pipeline_name': 'E-commerce ETL Pipeline',\n",
    "    'execution_date': datetime.now().isoformat(),\n",
    "    'data_sources': {\n",
    "        'sales': {'rows_raw': len(sales_df), 'rows_clean': len(sales_clean)},\n",
    "        'customers': {'rows_raw': len(customers_df), 'rows_clean': len(customers_clean)},\n",
    "        'products': {'rows': len(products_df)},\n",
    "        'inventory': {'rows_raw': len(inventory_df), 'rows_clean': len(inventory_clean)},\n",
    "        'suppliers': {'rows': len(suppliers_df)}\n",
    "    },\n",
    "    'validation_summary': {\n",
    "        'sales': sales_validator.get_summary(),\n",
    "        'customers': customers_validator.get_summary(),\n",
    "        'inventory': inventory_validator.get_summary()\n",
    "    },\n",
    "    'transformation_summary': {\n",
    "        'sales': sales_transformer.get_summary(),\n",
    "        'customers': customers_transformer.get_summary(),\n",
    "        'inventory': inventory_transformer.get_summary()\n",
    "    },\n",
    "    'output_files': [\n",
    "        'sales_cleaned.csv',\n",
    "        'sales_enriched.csv', \n",
    "        'customers_cleaned.csv',\n",
    "        'inventory_cleaned.csv',\n",
    "        'inventory_summary.csv',\n",
    "        'category_analytics.csv'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save report\n",
    "with open(PROCESSED_DATA_PATH / 'pipeline_report.json', 'w') as f:\n",
    "    json.dump(pipeline_report, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved: pipeline_report.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Pipeline Orchestration\n",
    "\n",
    "Demonstrate the complete automated pipeline using `ETLPipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create automated pipeline\n",
    "print(\"\\nüöÄ AUTOMATED ETL PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize pipeline\n",
    "etl = ETLPipeline(\n",
    "    name=\"EcommercePipeline\",\n",
    "    input_path=str(RAW_DATA_PATH),\n",
    "    output_path=str(PROCESSED_DATA_PATH)\n",
    ")\n",
    "\n",
    "# Define extraction steps\n",
    "etl.add_extract_csv('sales', 'sales_transactions.csv', parse_dates=['transaction_date'])\n",
    "etl.add_extract_csv('customers', 'customers.csv')\n",
    "etl.add_extract_json('products', 'product_catalog.json', normalize=True, record_path='products')\n",
    "\n",
    "# Define validation\n",
    "sales_val = DataValidator(name=\"AutoSalesValidator\")\n",
    "sales_val.add_null_check(['transaction_id', 'customer_id'])\n",
    "sales_val.add_range_check('quantity', min_val=0)\n",
    "etl.add_validation('sales', sales_val, fail_on_error=False)\n",
    "\n",
    "# Define transformations\n",
    "def clean_sales(df):\n",
    "    df = df.drop_duplicates(subset=['transaction_id'])\n",
    "    df = df[df['quantity'] > 0]\n",
    "    df = df[df['unit_price'] > 0]\n",
    "    df['total'] = df['quantity'] * df['unit_price']\n",
    "    return df\n",
    "\n",
    "etl.add_transform('sales', clean_sales, description=\"Clean Sales\")\n",
    "\n",
    "# Define output\n",
    "etl.add_load_csv('sales', 'auto_sales_processed.csv')\n",
    "\n",
    "# Run pipeline\n",
    "result = etl.run()\n",
    "\n",
    "print(f\"\\nüìä PIPELINE RESULT\")\n",
    "print(f\"Status: {result.status.value}\")\n",
    "print(f\"Duration: {result.duration_seconds:.2f} seconds\")\n",
    "print(f\"Steps completed: {result.steps_completed}/{result.steps_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Conclusions\n",
    "\n",
    "### Summary\n",
    "\n",
    "This project demonstrated a production-grade ETL pipeline that:\n",
    "\n",
    "1. **Extracted** data from 6 different sources (CSV, JSON, simulated API)\n",
    "2. **Validated** data quality with 15+ validation rules\n",
    "3. **Transformed** and cleaned data, handling:\n",
    "   - Duplicate records (50+ removed)\n",
    "   - Missing values (100+ handled)\n",
    "   - Invalid data (negative values, bad formats)\n",
    "4. **Integrated** data from multiple sources into enriched datasets\n",
    "5. **Generated** business analytics and insights\n",
    "6. **Exported** clean, production-ready datasets\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Data Sources | 6 |\n",
    "| Total Records Processed | 7,000+ |\n",
    "| Validation Checks | 15+ |\n",
    "| Data Quality Issues Fixed | 200+ |\n",
    "| Output Datasets | 6 |\n",
    "\n",
    "### Technical Highlights\n",
    "\n",
    "- **Modular Design**: Separate classes for Loading, Validation, Transformation, and Orchestration\n",
    "- **Chainable Methods**: Fluent API for building transformation pipelines\n",
    "- **Comprehensive Logging**: Full audit trail of all operations\n",
    "- **Error Handling**: Graceful handling of data quality issues\n",
    "- **Reusable Components**: Can be adapted for any ETL workflow\n",
    "\n",
    "### Business Impact\n",
    "\n",
    "- Clean data enables accurate reporting and analytics\n",
    "- Integrated datasets support cross-functional analysis\n",
    "- Automated pipeline reduces manual data processing time by 90%+\n",
    "- Validation rules prevent bad data from entering downstream systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"           MULTI-SOURCE ETL PIPELINE - COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüì• Data Sources Processed: 6\")\n",
    "print(f\"üìä Total Records: {len(sales_df) + len(customers_df) + len(products_df) + len(inventory_df):,}\")\n",
    "print(f\"‚úÖ Validation Checks: 15+\")\n",
    "print(f\"üîÑ Transformations Applied: {len(sales_transformer.log) + len(customers_transformer.log) + len(inventory_transformer.log)}\")\n",
    "print(f\"üíæ Output Files Generated: 6\")\n",
    "print(f\"\\nüìÅ Output Location: {PROCESSED_DATA_PATH}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
