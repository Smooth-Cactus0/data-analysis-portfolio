{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Motor Imagery Classification & Neural State Prediction\n",
    "\n",
    "**Author:** Alexy Louis  \n",
    "**Project:** Brain-Computer Interface (BCI) Signal Analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates a complete pipeline for analyzing EEG signals from motor imagery tasks. We will:\n",
    "\n",
    "1. **Load and explore** the PhysioNet EEG Motor Movement/Imagery Dataset\n",
    "2. **Preprocess** the signals (filtering, artifact removal, re-referencing)\n",
    "3. **Extract features** (time-domain, frequency-domain, and spatial features)\n",
    "4. **Classify** motor imagery tasks using classical ML and deep learning\n",
    "5. **Predict** neural states using sequence models\n",
    "6. **Visualize** brain activity patterns\n",
    "\n",
    "### Dataset\n",
    "\n",
    "- **Source:** PhysioNet EEG Motor Movement/Imagery Dataset\n",
    "- **Subjects:** 109 volunteers (we use 10 for this demo)\n",
    "- **Channels:** 64 EEG electrodes (10-20 system)\n",
    "- **Tasks:** Motor imagery (left/right hand), Real movement, Rest\n",
    "- **Sampling Rate:** 160 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# EEG analysis\n",
    "import mne\n",
    "from mne.datasets import eegbci\n",
    "from mne.io import read_raw_edf, concatenate_raws\n",
    "from mne.decoding import CSP\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Set up paths\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Local modules\n",
    "from src.preprocessing import (\n",
    "    load_subject_data, set_montage, apply_filters,\n",
    "    set_reference, create_epochs, preprocess_pipeline\n",
    ")\n",
    "from src.features import (\n",
    "    extract_time_features, extract_band_power,\n",
    "    extract_frequency_features, fit_csp,\n",
    "    extract_features_from_epochs, scale_features, FREQ_BANDS\n",
    ")\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "mne.set_log_level('WARNING')\n",
    "\n",
    "print(f\"MNE version: {mne.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Data Loading and Exploration\n",
    "\n",
    "### 1.1 Load EEG Data\n",
    "\n",
    "We'll load data from the PhysioNet EEG Motor Movement/Imagery Dataset. The dataset includes:\n",
    "- **Runs 4, 8, 12:** Motor imagery (left/right hand)\n",
    "- **Runs 3, 7, 11:** Real movement (left/right hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SUBJECT = 1  # Subject ID (1-109)\n",
    "RUNS_IMAGERY = [4, 8, 12]  # Motor imagery runs\n",
    "RUNS_MOVEMENT = [3, 7, 11]  # Real movement runs\n",
    "\n",
    "# Load motor imagery data\n",
    "print(f\"Loading data for Subject {SUBJECT}...\")\n",
    "raw = load_subject_data(SUBJECT, RUNS_IMAGERY)\n",
    "\n",
    "print(f\"\\nData loaded successfully!\")\n",
    "print(f\"  Channels: {len(raw.ch_names)}\")\n",
    "print(f\"  Sampling rate: {raw.info['sfreq']} Hz\")\n",
    "print(f\"  Duration: {raw.times[-1]:.1f} seconds\")\n",
    "print(f\"  Channel names: {raw.ch_names[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display raw data info\n",
    "print(raw.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Visualize Raw Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a segment of raw data\n",
    "fig, axes = plt.subplots(8, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Select channels of interest (sensorimotor cortex)\n",
    "channels = ['Fc3', 'Fc4', 'C3', 'Cz', 'C4', 'Cp3', 'Cp4', 'Pz']\n",
    "channels = [ch for ch in channels if ch in raw.ch_names][:8]\n",
    "\n",
    "start_time = 10  # Start at 10 seconds\n",
    "duration = 5  # 5 seconds\n",
    "sfreq = raw.info['sfreq']\n",
    "\n",
    "start_idx = int(start_time * sfreq)\n",
    "end_idx = int((start_time + duration) * sfreq)\n",
    "times = np.arange(start_idx, end_idx) / sfreq\n",
    "\n",
    "for i, (ax, ch) in enumerate(zip(axes, channels)):\n",
    "    ch_idx = raw.ch_names.index(ch)\n",
    "    data = raw.get_data()[ch_idx, start_idx:end_idx] * 1e6  # Convert to microvolts\n",
    "    ax.plot(times, data, 'b-', linewidth=0.5)\n",
    "    ax.set_ylabel(ch, rotation=0, ha='right', fontsize=10)\n",
    "    ax.set_ylim([-100, 100])\n",
    "    if i < len(channels) - 1:\n",
    "        ax.set_xticklabels([])\n",
    "\n",
    "axes[-1].set_xlabel('Time (s)')\n",
    "axes[0].set_title('Raw EEG Signals (Sensorimotor Cortex)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/raw_signals.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Explore Events/Annotations\n",
    "\n",
    "The dataset uses annotations to mark task events:\n",
    "- **T0:** Rest\n",
    "- **T1:** Left hand motor imagery/movement\n",
    "- **T2:** Right hand motor imagery/movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract events from annotations\n",
    "events, event_id = mne.events_from_annotations(raw)\n",
    "\n",
    "print(\"Event mapping:\")\n",
    "for name, code in event_id.items():\n",
    "    count = np.sum(events[:, 2] == code)\n",
    "    print(f\"  {name}: Code {code}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize event timeline\n",
    "fig, ax = plt.subplots(figsize=(14, 3))\n",
    "\n",
    "colors = {'T0': 'gray', 'T1': 'green', 'T2': 'red'}\n",
    "for name, code in event_id.items():\n",
    "    event_times = events[events[:, 2] == code, 0] / sfreq\n",
    "    ax.scatter(event_times, [code] * len(event_times), \n",
    "               c=colors.get(name, 'blue'), label=name, s=50, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Event Code')\n",
    "ax.set_title('Event Timeline')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Preprocessing Pipeline\n",
    "\n",
    "EEG signals require careful preprocessing to remove noise and artifacts:\n",
    "\n",
    "1. **Set electrode montage** for spatial information\n",
    "2. **Bandpass filter** (1-40 Hz) to remove drift and high-frequency noise\n",
    "3. **Re-reference** to average reference\n",
    "4. **Create epochs** around events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up montage\n",
    "montage = mne.channels.make_standard_montage('standard_1005')\n",
    "raw.set_montage(montage)\n",
    "\n",
    "# Plot sensor positions\n",
    "fig = raw.plot_sensors(show_names=True, sphere='auto')\n",
    "plt.title('EEG Electrode Positions (10-20 System)')\n",
    "plt.savefig('../images/electrode_positions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply bandpass filter\n",
    "print(\"Applying bandpass filter (1-40 Hz)...\")\n",
    "raw_filtered = raw.copy()\n",
    "raw_filtered.filter(l_freq=1.0, h_freq=40.0, fir_design='firwin')\n",
    "\n",
    "# Compare PSD before and after filtering\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Before filtering\n",
    "raw.compute_psd(fmax=80).plot(axes=axes[0], show=False)\n",
    "axes[0].set_title('Before Filtering')\n",
    "\n",
    "# After filtering\n",
    "raw_filtered.compute_psd(fmax=80).plot(axes=axes[1], show=False)\n",
    "axes[1].set_title('After Filtering (1-40 Hz)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/filtering_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set average reference\n",
    "print(\"Setting average reference...\")\n",
    "raw_filtered.set_eeg_reference('average', projection=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create epochs\n",
    "print(\"Creating epochs...\")\n",
    "\n",
    "# Event IDs for motor imagery\n",
    "event_id_motor = {'T1': 2, 'T2': 3}  # Left and Right hand\n",
    "\n",
    "epochs = mne.Epochs(\n",
    "    raw_filtered,\n",
    "    events,\n",
    "    event_id=event_id_motor,\n",
    "    tmin=-0.5,  # 0.5s before event\n",
    "    tmax=4.0,   # 4s after event\n",
    "    baseline=(-0.5, 0),\n",
    "    preload=True,\n",
    "    picks='eeg'\n",
    ")\n",
    "\n",
    "print(f\"\\nCreated {len(epochs)} epochs\")\n",
    "print(f\"  Left hand (T1): {len(epochs['T1'])} epochs\")\n",
    "print(f\"  Right hand (T2): {len(epochs['T2'])} epochs\")\n",
    "print(f\"  Time window: [{epochs.tmin}, {epochs.tmax}] seconds\")\n",
    "print(f\"  Epoch shape: {epochs.get_data().shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. EEG Data Visualization\n",
    "\n",
    "### 3.1 Power Spectral Density (PSD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare PSD between conditions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = {'T1': '#2ecc71', 'T2': '#e74c3c'}\n",
    "labels = {'T1': 'Left Hand', 'T2': 'Right Hand'}\n",
    "\n",
    "for event_name in ['T1', 'T2']:\n",
    "    epochs_cond = epochs[event_name]\n",
    "    spectrum = epochs_cond.compute_psd(method='welch', fmin=1, fmax=40)\n",
    "    psds, freqs = spectrum.get_data(return_freqs=True)\n",
    "    \n",
    "    # Average over epochs and channels\n",
    "    psds_mean = psds.mean(axis=(0, 1)) * 1e12\n",
    "    psds_std = psds.std(axis=(0, 1)) * 1e12\n",
    "    \n",
    "    # Linear scale\n",
    "    axes[0].plot(freqs, psds_mean, color=colors[event_name], \n",
    "                 label=labels[event_name], linewidth=2)\n",
    "    axes[0].fill_between(freqs, psds_mean - psds_std, psds_mean + psds_std,\n",
    "                         color=colors[event_name], alpha=0.2)\n",
    "    \n",
    "    # Log scale\n",
    "    axes[1].semilogy(freqs, psds_mean, color=colors[event_name],\n",
    "                     label=labels[event_name], linewidth=2)\n",
    "\n",
    "# Add frequency band annotations\n",
    "bands = {'Delta': (1, 4), 'Theta': (4, 8), 'Alpha': (8, 13), 'Beta': (13, 30)}\n",
    "for ax in axes:\n",
    "    for band_name, (fmin, fmax) in bands.items():\n",
    "        ax.axvspan(fmin, fmax, alpha=0.1, color='gray')\n",
    "\n",
    "axes[0].set_xlabel('Frequency (Hz)')\n",
    "axes[0].set_ylabel('Power (uV^2/Hz)')\n",
    "axes[0].set_title('PSD - Linear Scale')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_xlabel('Frequency (Hz)')\n",
    "axes[1].set_ylabel('Power (uV^2/Hz)')\n",
    "axes[1].set_title('PSD - Log Scale')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/psd_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Event-Related Potentials (ERPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ERPs for motor cortex channels\n",
    "motor_channels = ['C3', 'Cz', 'C4']\n",
    "motor_channels = [ch for ch in motor_channels if ch in epochs.ch_names]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(motor_channels), figsize=(14, 4))\n",
    "times = epochs.times\n",
    "\n",
    "for ax, ch_name in zip(axes, motor_channels):\n",
    "    ch_idx = epochs.ch_names.index(ch_name)\n",
    "    \n",
    "    for event_name in ['T1', 'T2']:\n",
    "        epochs_cond = epochs[event_name]\n",
    "        data = epochs_cond.get_data()[:, ch_idx, :]\n",
    "        \n",
    "        mean = data.mean(axis=0) * 1e6\n",
    "        sem = data.std(axis=0) / np.sqrt(len(data)) * 1e6\n",
    "        \n",
    "        ax.plot(times, mean, color=colors[event_name], \n",
    "                label=labels[event_name], linewidth=2)\n",
    "        ax.fill_between(times, mean - sem, mean + sem,\n",
    "                        color=colors[event_name], alpha=0.2)\n",
    "    \n",
    "    ax.axvline(0, color='k', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    ax.axhline(0, color='k', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Amplitude (uV)')\n",
    "    ax.set_title(f'Channel {ch_name}')\n",
    "    ax.legend()\n",
    "\n",
    "fig.suptitle('Event-Related Potentials (Motor Cortex)', fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/erp_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Topographic Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topographic maps at different time points\n",
    "time_points = [0.5, 1.0, 2.0, 3.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, len(time_points), figsize=(14, 7))\n",
    "\n",
    "for row, event_name in enumerate(['T1', 'T2']):\n",
    "    epochs_cond = epochs[event_name]\n",
    "    evoked = epochs_cond.average()\n",
    "    \n",
    "    for col, t in enumerate(time_points):\n",
    "        evoked.plot_topomap(times=t, axes=axes[row, col], show=False,\n",
    "                            colorbar=False, time_unit='s')\n",
    "        if row == 0:\n",
    "            axes[row, col].set_title(f't = {t}s')\n",
    "    \n",
    "    axes[row, 0].set_ylabel(labels[event_name], fontsize=12)\n",
    "\n",
    "fig.suptitle('Topographic Maps Over Time', fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/topomaps.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Time-Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-frequency analysis using Morlet wavelets\n",
    "from mne.time_frequency import tfr_morlet\n",
    "\n",
    "freqs = np.arange(4, 35, 1)\n",
    "n_cycles = freqs / 2\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "for row, event_name in enumerate(['T1', 'T2']):\n",
    "    epochs_cond = epochs[event_name]\n",
    "    \n",
    "    for col, ch_name in enumerate(['C3', 'Cz', 'C4']):\n",
    "        if ch_name not in epochs.ch_names:\n",
    "            continue\n",
    "            \n",
    "        power = tfr_morlet(\n",
    "            epochs_cond, freqs=freqs, n_cycles=n_cycles,\n",
    "            return_itc=False, picks=ch_name, average=True\n",
    "        )\n",
    "        \n",
    "        # Baseline normalize\n",
    "        power.apply_baseline(baseline=(-0.5, 0), mode='percent')\n",
    "        \n",
    "        power.plot(\n",
    "            picks=ch_name, axes=axes[row, col],\n",
    "            show=False, colorbar=False,\n",
    "            title=f'{labels[event_name]} - {ch_name}'\n",
    "        )\n",
    "\n",
    "fig.suptitle('Time-Frequency Representations (% change from baseline)', fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/time_frequency.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Feature Extraction\n",
    "\n",
    "We'll extract three types of features:\n",
    "1. **Time-domain:** Statistical measures (mean, variance, etc.)\n",
    "2. **Frequency-domain:** Band power (alpha, beta, etc.)\n",
    "3. **Spatial:** Common Spatial Patterns (CSP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels\n",
    "labels_array = epochs.events[:, -1]\n",
    "print(f\"Labels: {np.unique(labels_array, return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from all epochs\n",
    "X, y, feature_names = extract_features_from_epochs(epochs)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"\\nSample feature names: {feature_names[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle any NaN or Inf values\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "X_train_scaled, X_test_scaled, scaler = scale_features(X_train, X_test)\n",
    "\n",
    "print(f\"Training set: {X_train_scaled.shape}\")\n",
    "print(f\"Test set: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Common Spatial Patterns (CSP)\n",
    "\n",
    "CSP is a powerful spatial filtering technique for EEG that maximizes the variance for one class while minimizing it for another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit CSP\n",
    "csp = CSP(n_components=6, reg='ledoit_wolf', log=True, norm_trace=False)\n",
    "csp.fit(epochs.get_data(), y)\n",
    "\n",
    "# Plot CSP patterns\n",
    "fig, axes = plt.subplots(1, 6, figsize=(14, 3))\n",
    "\n",
    "for idx, ax in enumerate(axes):\n",
    "    mne.viz.plot_topomap(\n",
    "        csp.patterns_[idx], epochs.info,\n",
    "        axes=ax, show=False, cmap='RdBu_r'\n",
    "    )\n",
    "    ax.set_title(f'CSP {idx+1}')\n",
    "\n",
    "fig.suptitle('Common Spatial Patterns', fontsize=12, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/csp_patterns.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract CSP features\n",
    "X_csp = csp.transform(epochs.get_data())\n",
    "\n",
    "# Visualize CSP features\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for label, color in zip([2, 3], ['green', 'red']):\n",
    "    mask = y == label\n",
    "    ax.scatter(X_csp[mask, 0], X_csp[mask, -1], c=color, alpha=0.7,\n",
    "               label='Left Hand' if label == 2 else 'Right Hand')\n",
    "\n",
    "ax.set_xlabel('CSP Component 1')\n",
    "ax.set_ylabel('CSP Component 6')\n",
    "ax.set_title('CSP Feature Space')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/csp_features.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Classification with Classical ML\n",
    "\n",
    "We'll compare several classical machine learning algorithms:\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "- Support Vector Machine (SVM)\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'LDA': LinearDiscriminantAnalysis(),\n",
    "    'SVM-RBF': SVC(kernel='rbf', C=1.0, probability=True),\n",
    "    'SVM-Linear': SVC(kernel='linear', C=1.0, probability=True),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = {}\n",
    "\n",
    "print(\"Cross-Validation Results (5-fold):\\n\")\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='accuracy')\n",
    "    results[name] = {'mean': scores.mean(), 'std': scores.std(), 'scores': scores}\n",
    "    print(f\"{name:15} Accuracy: {scores.mean():.3f} (+/- {scores.std():.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train best model and evaluate on test set\n",
    "best_model_name = max(results, key=lambda x: results[x]['mean'])\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "\n",
    "best_model = models[best_model_name]\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Left Hand', 'Right Hand']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=['Left Hand', 'Right Hand'],\n",
    "            yticklabels=['Left Hand', 'Right Hand'], ax=ax)\n",
    "\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title(f'Confusion Matrix ({best_model_name})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "model_names = list(results.keys())\n",
    "means = [results[name]['mean'] for name in model_names]\n",
    "stds = [results[name]['std'] for name in model_names]\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(model_names)))\n",
    "bars = ax.bar(model_names, means, yerr=stds, color=colors, edgecolor='black', capsize=5)\n",
    "\n",
    "ax.axhline(0.5, color='red', linestyle='--', label='Chance level')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Model Comparison - Cross-Validation Accuracy')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels\n",
    "for bar, mean in zip(bars, means):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "            f'{mean:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Classification with CSP + LDA Pipeline\n",
    "\n",
    "The CSP + LDA pipeline is a classic approach for motor imagery BCI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create CSP + LDA pipeline\n",
    "csp_lda_pipeline = Pipeline([\n",
    "    ('csp', CSP(n_components=6, reg='ledoit_wolf', log=True)),\n",
    "    ('lda', LinearDiscriminantAnalysis())\n",
    "])\n",
    "\n",
    "# Get epoch data\n",
    "X_epochs = epochs.get_data()\n",
    "\n",
    "# Split\n",
    "X_train_ep, X_test_ep, y_train_ep, y_test_ep = train_test_split(\n",
    "    X_epochs, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "scores_csp_lda = cross_val_score(csp_lda_pipeline, X_train_ep, y_train_ep, cv=cv)\n",
    "print(f\"CSP + LDA Cross-Validation Accuracy: {scores_csp_lda.mean():.3f} (+/- {scores_csp_lda.std():.3f})\")\n",
    "\n",
    "# Train and evaluate\n",
    "csp_lda_pipeline.fit(X_train_ep, y_train_ep)\n",
    "y_pred_csp = csp_lda_pipeline.predict(X_test_ep)\n",
    "print(f\"CSP + LDA Test Accuracy: {accuracy_score(y_test_ep, y_pred_csp):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Multi-Subject Analysis\n",
    "\n",
    "Let's analyze multiple subjects to understand variability across individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze multiple subjects\n",
    "subjects = [1, 2, 3, 4, 5]\n",
    "subject_results = []\n",
    "\n",
    "print(\"Processing multiple subjects...\\n\")\n",
    "\n",
    "for subj in subjects:\n",
    "    try:\n",
    "        # Load and preprocess\n",
    "        raw_subj = load_subject_data(subj, RUNS_IMAGERY)\n",
    "        raw_subj.set_montage(mne.channels.make_standard_montage('standard_1005'))\n",
    "        raw_subj.filter(l_freq=1.0, h_freq=40.0, fir_design='firwin')\n",
    "        raw_subj.set_eeg_reference('average', projection=False)\n",
    "        \n",
    "        # Create epochs\n",
    "        events_subj, _ = mne.events_from_annotations(raw_subj)\n",
    "        epochs_subj = mne.Epochs(\n",
    "            raw_subj, events_subj, event_id=event_id_motor,\n",
    "            tmin=-0.5, tmax=4.0, baseline=(-0.5, 0),\n",
    "            preload=True, picks='eeg'\n",
    "        )\n",
    "        \n",
    "        # Extract data\n",
    "        X_subj = epochs_subj.get_data()\n",
    "        y_subj = epochs_subj.events[:, -1]\n",
    "        \n",
    "        # CSP + LDA pipeline\n",
    "        scores = cross_val_score(csp_lda_pipeline, X_subj, y_subj, cv=5)\n",
    "        \n",
    "        subject_results.append({\n",
    "            'subject': subj,\n",
    "            'n_epochs': len(epochs_subj),\n",
    "            'accuracy_mean': scores.mean(),\n",
    "            'accuracy_std': scores.std()\n",
    "        })\n",
    "        \n",
    "        print(f\"Subject {subj}: Accuracy = {scores.mean():.3f} (+/- {scores.std():.3f})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Subject {subj}: Error - {e}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(subject_results)\n",
    "print(f\"\\nMean accuracy across subjects: {df_results['accuracy_mean'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot subject variability\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(df_results)))\n",
    "bars = ax.bar(df_results['subject'].astype(str), df_results['accuracy_mean'],\n",
    "              yerr=df_results['accuracy_std'], color=colors, edgecolor='black', capsize=5)\n",
    "\n",
    "ax.axhline(0.5, color='red', linestyle='--', label='Chance level')\n",
    "ax.axhline(df_results['accuracy_mean'].mean(), color='blue', linestyle='-.',\n",
    "           label=f'Mean = {df_results[\"accuracy_mean\"].mean():.3f}')\n",
    "\n",
    "ax.set_xlabel('Subject ID')\n",
    "ax.set_ylabel('Classification Accuracy')\n",
    "ax.set_title('Motor Imagery Classification - Subject Variability')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../images/subject_variability.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Summary and Conclusions\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **EEG signals contain discriminative information** for distinguishing left vs right hand motor imagery\n",
    "2. **Preprocessing is crucial:** Filtering and artifact removal significantly improve signal quality\n",
    "3. **CSP is highly effective** for motor imagery classification, extracting spatial patterns that maximize class separability\n",
    "4. **Inter-subject variability** is significant, highlighting the need for subject-specific calibration\n",
    "5. **Classical ML achieves reasonable accuracy** for this binary classification task\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Deep learning approaches (EEGNet, CNN)\n",
    "- Neural state prediction using RNNs\n",
    "- Real-time BCI implementation\n",
    "- Transfer learning across subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary statistics\n",
    "summary = {\n",
    "    'dataset': 'PhysioNet EEG Motor Imagery',\n",
    "    'n_subjects_analyzed': len(df_results),\n",
    "    'n_channels': len(epochs.ch_names),\n",
    "    'sampling_rate': epochs.info['sfreq'],\n",
    "    'best_single_subject': df_results.loc[df_results['accuracy_mean'].idxmax()].to_dict(),\n",
    "    'mean_accuracy': df_results['accuracy_mean'].mean(),\n",
    "    'std_accuracy': df_results['accuracy_std'].mean(),\n",
    "    'model': 'CSP + LDA'\n",
    "}\n",
    "\n",
    "print(\"Summary Statistics:\")\n",
    "for key, value in summary.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EEG Motor Imagery Analysis Complete!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
