{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transient Classification from Light Curves\n",
        "\n",
        "This notebook demonstrates the classification of astronomical transients and variable stars using deep learning on time-series photometry (light curves).\n",
        "\n",
        "## The Science of Transient Astronomy\n",
        "\n",
        "**Transient astronomy** studies objects that change brightness over time. These include:\n",
        "\n",
        "### Explosive Transients (Single Events)\n",
        "\n",
        "| Type | Physical Origin | Timescale | Scientific Importance |\n",
        "|------|----------------|-----------|----------------------|\n",
        "| **Type Ia Supernova** | White dwarf thermonuclear explosion | ~30 days rise, months decline | Cosmological distance ladder, dark energy discovery |\n",
        "| **Type II Supernova** | Massive star core collapse | ~100 days plateau | Nucleosynthesis, neutron star formation |\n",
        "| **Kilonova** | Neutron star merger | ~1-2 days | Heavy element production (gold, platinum), gravitational waves |\n",
        "| **TDE** | Star destroyed by black hole | Weeks to months | Black hole demographics, accretion physics |\n",
        "| **SLSN** | Unknown (magnetar?) | Months | Extreme stellar explosions, early universe |\n",
        "\n",
        "### Variable Stars (Periodic/Recurring)\n",
        "\n",
        "| Type | Physical Origin | Period | Scientific Importance |\n",
        "|------|----------------|--------|----------------------|\n",
        "| **RR Lyrae** | Radial pulsation | 0.2-1 day | Distance indicators, old stellar populations |\n",
        "| **Cepheids** | Radial pulsation | 1-100 days | Primary distance ladder rung |\n",
        "| **Eclipsing Binaries** | Stellar eclipses | Hours to days | Stellar masses and radii |\n",
        "| **AGN** | Black hole accretion | Stochastic | Supermassive black holes, galaxy evolution |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, str(Path('.').resolve().parent))\n",
        "\n",
        "# Deep learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Our modules\n",
        "from src.preprocessing import LightCurvePreprocessor\n",
        "from src.models import TransientCNN1D, TransientLSTM, HybridCNNLSTM, create_classical_pipeline\n",
        "from src.visualization import plot_light_curve, plot_confusion_matrix, plot_training_history\n",
        "\n",
        "# Settings\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Passband colors (LSST ugrizy)\n",
        "PASSBAND_COLORS = {\n",
        "    0: '#8b5cf6',  # u - ultraviolet\n",
        "    1: '#3b82f6',  # g - blue\n",
        "    2: '#22c55e',  # r - green\n",
        "    3: '#f97316',  # i - orange\n",
        "    4: '#ef4444',  # z - red\n",
        "    5: '#7f1d1d',  # y - dark red\n",
        "}\n",
        "PASSBAND_NAMES = {0: 'u', 1: 'g', 2: 'r', 3: 'i', 4: 'z', 5: 'y'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Understanding Light Curves\n",
        "\n",
        "A **light curve** is a time series of brightness measurements. In modern surveys:\n",
        "\n",
        "- **Multi-band photometry**: Measurements in multiple filters (colors)\n",
        "- **Non-uniform sampling**: Observations depend on weather, moon phase, telescope scheduling\n",
        "- **Varying uncertainties**: Brighter objects have lower relative errors\n",
        "\n",
        "### Why Multi-Band Matters\n",
        "\n",
        "Different transient types have characteristic **colors** (flux ratios between bands):\n",
        "\n",
        "- **Kilonovae**: Very red (heavy element emission)\n",
        "- **Type Ia SNe**: Blue at peak, reddens over time\n",
        "- **AGN**: Relatively constant color with varying brightness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic light curve data\n",
        "np.random.seed(42)\n",
        "\n",
        "def generate_light_curve(class_id, object_id):\n",
        "    \"\"\"Generate a synthetic light curve for a given transient class.\"\"\"\n",
        "    n_obs = np.random.randint(60, 150)\n",
        "    mjd = np.sort(np.random.uniform(59000, 60000, n_obs))\n",
        "    passbands = np.random.choice([0, 1, 2, 3, 4, 5], n_obs)\n",
        "    \n",
        "    # Class-specific light curve shapes\n",
        "    if class_id == 0:  # SNIa\n",
        "        peak_mjd = np.random.uniform(59300, 59700)\n",
        "        t = mjd - peak_mjd\n",
        "        peak_flux = np.random.uniform(500, 2000)\n",
        "        flux = peak_flux * np.exp(-0.5 * (t/15)**2) * (t < 100)\n",
        "        flux += peak_flux * 0.3 * np.exp(-t/40) * (t > 0) * (t < 100)\n",
        "        \n",
        "    elif class_id == 1:  # SNII\n",
        "        peak_mjd = np.random.uniform(59300, 59700)\n",
        "        t = mjd - peak_mjd\n",
        "        peak_flux = np.random.uniform(300, 1500)\n",
        "        flux = peak_flux * np.exp(-0.5 * (t/20)**2) * (t < 50)\n",
        "        flux += peak_flux * 0.5 * (t >= 50) * (t < 100)  # Plateau\n",
        "        flux *= np.exp(-t/100) * (t > 0)\n",
        "        \n",
        "    elif class_id == 2:  # Kilonova\n",
        "        peak_mjd = np.random.uniform(59300, 59700)\n",
        "        t = mjd - peak_mjd\n",
        "        peak_flux = np.random.uniform(200, 800)\n",
        "        flux = peak_flux * np.exp(-t/2) * (t > 0) * (t < 20)  # Very fast!\n",
        "        \n",
        "    elif class_id == 3:  # RR Lyrae\n",
        "        period = np.random.uniform(0.4, 0.9)\n",
        "        amplitude = np.random.uniform(100, 300)\n",
        "        flux = 500 + amplitude * np.sin(2 * np.pi * mjd / period)\n",
        "        \n",
        "    elif class_id == 4:  # AGN\n",
        "        base_flux = np.random.uniform(200, 800)\n",
        "        flux = base_flux + 100 * np.cumsum(np.random.randn(n_obs)) / np.sqrt(n_obs)\n",
        "        \n",
        "    else:  # Eclipsing Binary\n",
        "        period = np.random.uniform(0.5, 5)\n",
        "        phase = (mjd % period) / period\n",
        "        flux = 800 - 200 * (np.abs(phase - 0.5) < 0.1).astype(float)  # Primary eclipse\n",
        "        flux -= 100 * (np.abs(phase) < 0.05).astype(float)  # Secondary eclipse\n",
        "    \n",
        "    flux = np.maximum(flux, 0)\n",
        "    flux_err = np.sqrt(flux + 100) * np.random.uniform(0.8, 1.2, n_obs)\n",
        "    flux += np.random.randn(n_obs) * flux_err * 0.5\n",
        "    \n",
        "    # Color dependence\n",
        "    color_offset = (passbands - 2) * 50 * np.random.uniform(0.5, 1.5)\n",
        "    flux += color_offset\n",
        "    flux = np.maximum(flux, 1)\n",
        "    \n",
        "    return pd.DataFrame({\n",
        "        'object_id': object_id,\n",
        "        'mjd': mjd,\n",
        "        'passband': passbands,\n",
        "        'flux': flux,\n",
        "        'flux_err': flux_err\n",
        "    })\n",
        "\n",
        "# Generate dataset\n",
        "class_names = ['SNIa', 'SNII', 'Kilonova', 'RRLyrae', 'AGN', 'EclipsingBinary']\n",
        "n_per_class = 100\n",
        "\n",
        "light_curves = []\n",
        "metadata = []\n",
        "\n",
        "object_id = 0\n",
        "for class_id, class_name in enumerate(class_names):\n",
        "    for _ in range(n_per_class):\n",
        "        lc = generate_light_curve(class_id, object_id)\n",
        "        light_curves.append(lc)\n",
        "        metadata.append({\n",
        "            'object_id': object_id,\n",
        "            'target': class_id,\n",
        "            'class_name': class_name\n",
        "        })\n",
        "        object_id += 1\n",
        "\n",
        "lc_df = pd.concat(light_curves, ignore_index=True)\n",
        "meta_df = pd.DataFrame(metadata)\n",
        "\n",
        "print(f\"Total light curves: {len(meta_df)}\")\n",
        "print(f\"Total observations: {len(lc_df)}\")\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(meta_df['class_name'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize example light curves for each class\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, class_name in enumerate(class_names):\n",
        "    ax = axes[i]\n",
        "    class_objects = meta_df[meta_df['class_name'] == class_name]['object_id'].values[:1]\n",
        "    \n",
        "    for obj_id in class_objects:\n",
        "        obj_lc = lc_df[lc_df['object_id'] == obj_id]\n",
        "        \n",
        "        for pb in sorted(obj_lc['passband'].unique()):\n",
        "            pb_data = obj_lc[obj_lc['passband'] == pb].sort_values('mjd')\n",
        "            ax.errorbar(\n",
        "                pb_data['mjd'], pb_data['flux'], \n",
        "                yerr=pb_data['flux_err'],\n",
        "                fmt='o', color=PASSBAND_COLORS[pb], \n",
        "                label=PASSBAND_NAMES[pb], alpha=0.7,\n",
        "                markersize=4, capsize=2\n",
        "            )\n",
        "    \n",
        "    ax.set_xlabel('MJD')\n",
        "    ax.set_ylabel('Flux')\n",
        "    ax.set_title(class_name, fontsize=12, fontweight='bold')\n",
        "    ax.legend(loc='best', fontsize=8)\n",
        "\n",
        "plt.suptitle('Example Light Curves by Transient Class', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../images/transient_examples.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preprocessing\n",
        "\n",
        "Light curves require special preprocessing:\n",
        "\n",
        "1. **Interpolation to regular grid**: Neural networks expect fixed-size input\n",
        "2. **Per-band processing**: Each filter has different sensitivity\n",
        "3. **Normalization**: Make comparison across objects possible\n",
        "\n",
        "### Why Interpolation?\n",
        "\n",
        "Real observations are **irregularly sampled**:\n",
        "- Weather cancellations\n",
        "- Moon phase (can't observe near bright moon)\n",
        "- Telescope scheduling (multiple programs share time)\n",
        "\n",
        "Neural networks need regular grids, so we interpolate to a fixed time grid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess light curves\n",
        "preprocessor = LightCurvePreprocessor(n_time_bins=100, passbands=[0,1,2,3,4,5])\n",
        "\n",
        "object_ids = meta_df['object_id'].values\n",
        "labels = meta_df['target'].values\n",
        "\n",
        "# Process all light curves\n",
        "X_interp = []\n",
        "X_features = []\n",
        "\n",
        "for obj_id in object_ids:\n",
        "    result = preprocessor.preprocess(lc_df, object_id=obj_id)\n",
        "    X_interp.append(result['interpolated'])\n",
        "    X_features.append(result['features'])\n",
        "\n",
        "X_interp = np.array(X_interp)  # Shape: (n_objects, n_bands, n_time)\n",
        "X_features = np.array(X_features)  # Shape: (n_objects, n_features)\n",
        "\n",
        "print(f\"Interpolated shape: {X_interp.shape}\")\n",
        "print(f\"Features shape: {X_features.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize interpolated light curve\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Original (irregular sampling)\n",
        "ax = axes[0]\n",
        "obj_id = 0\n",
        "obj_lc = lc_df[lc_df['object_id'] == obj_id]\n",
        "for pb in sorted(obj_lc['passband'].unique()):\n",
        "    pb_data = obj_lc[obj_lc['passband'] == pb].sort_values('mjd')\n",
        "    ax.scatter(pb_data['mjd'], pb_data['flux'], \n",
        "               color=PASSBAND_COLORS[pb], label=PASSBAND_NAMES[pb], s=20)\n",
        "ax.set_xlabel('MJD')\n",
        "ax.set_ylabel('Flux')\n",
        "ax.set_title('Original Light Curve (Irregular Sampling)')\n",
        "ax.legend()\n",
        "\n",
        "# Interpolated (regular grid)\n",
        "ax = axes[1]\n",
        "for pb in range(6):\n",
        "    ax.plot(X_interp[obj_id, pb], color=PASSBAND_COLORS[pb], \n",
        "            label=PASSBAND_NAMES[pb], linewidth=1.5)\n",
        "ax.set_xlabel('Time Bin')\n",
        "ax.set_ylabel('Normalized Flux')\n",
        "ax.set_title('Interpolated Light Curve (Regular Grid)')\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../images/lc_preprocessing.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Architecture Comparison\n",
        "\n",
        "We compare three approaches for light curve classification:\n",
        "\n",
        "### 1D-CNN: Local Feature Detection\n",
        "- Detects **local temporal patterns**: rise shape, peak shape, decline rate\n",
        "- Translation-invariant: can detect features at any time\n",
        "\n",
        "### LSTM: Sequential Dependencies\n",
        "- Models **long-range dependencies**: early behavior predicts late behavior\n",
        "- Memory cells preserve information across long sequences\n",
        "- Bidirectional: sees both past and future context\n",
        "\n",
        "### Hybrid CNN-LSTM: Best of Both\n",
        "- CNN extracts local features\n",
        "- LSTM models relationships between features over time\n",
        "\n",
        "### Why NOT 2D-CNN?\n",
        "\n",
        "Light curves are **1D time series**, not images!\n",
        "\n",
        "- 2D-CNNs would require creating artificial images (e.g., spectrograms)\n",
        "- This loses the natural temporal ordering\n",
        "- 1D-CNNs directly process the sequential nature of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_interp, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_torch = torch.FloatTensor(X_train)\n",
        "X_test_torch = torch.FloatTensor(X_test)\n",
        "y_train_torch = torch.LongTensor(y_train)\n",
        "y_test_torch = torch.LongTensor(y_test)\n",
        "\n",
        "# Data loaders\n",
        "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
        "test_dataset = TensorDataset(X_test_torch, y_test_torch)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define training function\n",
        "def train_model(model, train_loader, test_loader, n_epochs=30, lr=0.001):\n",
        "    \"\"\"Train a model and return history.\"\"\"\n",
        "    model = model.to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss, train_correct, train_total = 0, 0, 0\n",
        "        \n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            _, pred = torch.max(outputs, 1)\n",
        "            train_total += batch_y.size(0)\n",
        "            train_correct += (pred == batch_y).sum().item()\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0, 0, 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in test_loader:\n",
        "                batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE)\n",
        "                outputs = model(batch_x)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                \n",
        "                val_loss += loss.item()\n",
        "                _, pred = torch.max(outputs, 1)\n",
        "                val_total += batch_y.size(0)\n",
        "                val_correct += (pred == batch_y).sum().item()\n",
        "        \n",
        "        history['train_loss'].append(train_loss / len(train_loader))\n",
        "        history['val_loss'].append(val_loss / len(test_loader))\n",
        "        history['train_acc'].append(train_correct / train_total)\n",
        "        history['val_acc'].append(val_correct / val_total)\n",
        "        \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}: Val Acc = {history['val_acc'][-1]:.4f}\")\n",
        "    \n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train 1D-CNN\n",
        "print(\"Training 1D-CNN...\")\n",
        "cnn_model = TransientCNN1D(n_classes=6, n_bands=6, n_time=100)\n",
        "cnn_model, cnn_history = train_model(cnn_model, train_loader, test_loader, n_epochs=30)\n",
        "\n",
        "print(f\"\\n1D-CNN Final Validation Accuracy: {cnn_history['val_acc'][-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train LSTM\n",
        "print(\"Training LSTM...\")\n",
        "lstm_model = TransientLSTM(n_classes=6, n_bands=6, hidden_size=128)\n",
        "lstm_model, lstm_history = train_model(lstm_model, train_loader, test_loader, n_epochs=30)\n",
        "\n",
        "print(f\"\\nLSTM Final Validation Accuracy: {lstm_history['val_acc'][-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Hybrid CNN-LSTM\n",
        "print(\"Training Hybrid CNN-LSTM...\")\n",
        "hybrid_model = HybridCNNLSTM(n_classes=6, n_bands=6)\n",
        "hybrid_model, hybrid_history = train_model(hybrid_model, train_loader, test_loader, n_epochs=30)\n",
        "\n",
        "print(f\"\\nHybrid CNN-LSTM Final Validation Accuracy: {hybrid_history['val_acc'][-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare models\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Validation loss\n",
        "ax = axes[0]\n",
        "ax.plot(cnn_history['val_loss'], label='1D-CNN', linewidth=2)\n",
        "ax.plot(lstm_history['val_loss'], label='LSTM', linewidth=2)\n",
        "ax.plot(hybrid_history['val_loss'], label='Hybrid CNN-LSTM', linewidth=2)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Validation Loss')\n",
        "ax.set_title('Model Comparison: Validation Loss')\n",
        "ax.legend()\n",
        "\n",
        "# Validation accuracy\n",
        "ax = axes[1]\n",
        "ax.plot(cnn_history['val_acc'], label='1D-CNN', linewidth=2)\n",
        "ax.plot(lstm_history['val_acc'], label='LSTM', linewidth=2)\n",
        "ax.plot(hybrid_history['val_acc'], label='Hybrid CNN-LSTM', linewidth=2)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Validation Accuracy')\n",
        "ax.set_title('Model Comparison: Validation Accuracy')\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../images/transient_model_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Summary\n",
        "print(\"\\nModel Comparison Summary:\")\n",
        "print(f\"  1D-CNN:         {max(cnn_history['val_acc']):.4f}\")\n",
        "print(f\"  LSTM:           {max(lstm_history['val_acc']):.4f}\")\n",
        "print(f\"  Hybrid CNN-LSTM: {max(hybrid_history['val_acc']):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Detailed Evaluation\n",
        "\n",
        "### Interpretation of Confusion Matrix\n",
        "\n",
        "Common confusions in transient classification:\n",
        "\n",
        "- **SNIa ↔ SNII**: Both are supernovae, but different physics\n",
        "- **RRLyrae ↔ Eclipsing Binary**: Both periodic, but different shapes\n",
        "- **Kilonova confusion**: Very fast, few observations, hard to classify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate best model (Hybrid)\n",
        "hybrid_model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_x, batch_y in test_loader:\n",
        "        batch_x = batch_x.to(DEVICE)\n",
        "        outputs = hybrid_model(batch_x)\n",
        "        _, pred = torch.max(outputs, 1)\n",
        "        all_preds.extend(pred.cpu().numpy())\n",
        "        all_labels.extend(batch_y.numpy())\n",
        "\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# Classification report\n",
        "print(\"Classification Report (Hybrid CNN-LSTM):\")\n",
        "print(classification_report(all_labels, all_preds, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "fig = plot_confusion_matrix(all_labels, all_preds, class_names=class_names)\n",
        "plt.savefig('../images/transient_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Classical ML Comparison\n",
        "\n",
        "Deep learning isn't always necessary. Classical ML with hand-crafted features often works well:\n",
        "\n",
        "**Advantages of Classical ML:**\n",
        "- Faster training\n",
        "- More interpretable\n",
        "- Works with less data\n",
        "\n",
        "**Advantages of Deep Learning:**\n",
        "- Learns features automatically\n",
        "- Can model complex patterns\n",
        "- Scales to large datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classical ML with extracted features\n",
        "X_feat_train, X_feat_test, y_feat_train, y_feat_test = train_test_split(\n",
        "    X_features, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "# Handle NaN values\n",
        "X_feat_train = np.nan_to_num(X_feat_train, nan=0.0)\n",
        "X_feat_test = np.nan_to_num(X_feat_test, nan=0.0)\n",
        "\n",
        "# Train Random Forest\n",
        "rf_pipeline = create_classical_pipeline('random_forest')\n",
        "rf_pipeline.fit(X_feat_train, y_feat_train)\n",
        "rf_score = rf_pipeline.score(X_feat_test, y_feat_test)\n",
        "\n",
        "print(f\"Random Forest Accuracy: {rf_score:.4f}\")\n",
        "\n",
        "# Train Gradient Boosting\n",
        "gb_pipeline = create_classical_pipeline('gradient_boosting')\n",
        "gb_pipeline.fit(X_feat_train, y_feat_train)\n",
        "gb_score = gb_pipeline.score(X_feat_test, y_feat_test)\n",
        "\n",
        "print(f\"Gradient Boosting Accuracy: {gb_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final comparison\n",
        "results = {\n",
        "    '1D-CNN': max(cnn_history['val_acc']),\n",
        "    'LSTM': max(lstm_history['val_acc']),\n",
        "    'Hybrid CNN-LSTM': max(hybrid_history['val_acc']),\n",
        "    'Random Forest': rf_score,\n",
        "    'Gradient Boosting': gb_score\n",
        "}\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "models = list(results.keys())\n",
        "scores = list(results.values())\n",
        "colors = ['#3498db', '#2ecc71', '#9b59b6', '#e74c3c', '#f39c12']\n",
        "\n",
        "bars = ax.barh(models, scores, color=colors)\n",
        "\n",
        "for bar, score in zip(bars, scores):\n",
        "    ax.text(score + 0.01, bar.get_y() + bar.get_height()/2,\n",
        "            f'{score:.3f}', va='center', fontsize=11)\n",
        "\n",
        "ax.set_xlabel('Accuracy', fontsize=12)\n",
        "ax.set_title('Transient Classification: Model Comparison', fontsize=14)\n",
        "ax.set_xlim(0, 1.1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../images/transient_final_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Conclusions\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "1. **Deep learning effectively classifies transients** from multi-band light curves\n",
        "2. **Hybrid CNN-LSTM** often performs best by combining local and global features\n",
        "3. **Classical ML with good features** can be competitive, especially with limited data\n",
        "4. **Rare transients** (kilonovae) are challenging due to fast evolution and few samples\n",
        "\n",
        "### Scientific Implications\n",
        "\n",
        "Automated classification enables:\n",
        "- **Real-time alerts** for follow-up observations\n",
        "- **Large-scale surveys** (LSST will produce 10M alerts/night)\n",
        "- **Discovery of rare events** that would be missed manually\n",
        "\n",
        "### Limitations\n",
        "\n",
        "- **Synthetic data**: Real light curves have more complexity\n",
        "- **Imbalanced classes**: Some transients are very rare\n",
        "- **Redshift effects**: Distant objects are fainter and time-dilated\n",
        "\n",
        "### Future Directions\n",
        "\n",
        "- Use real PLAsTiCC/ZTF data with 14+ classes\n",
        "- Incorporate host galaxy information\n",
        "- Attention mechanisms for interpretability\n",
        "- Active learning for rare transient discovery"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
